{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jil_2aqplrgb"
      },
      "source": [
        "# **Tutorial: Best Hands-on Unstructured Data Analysis using PySpark RDD and DF**\n",
        "\n",
        "\n",
        "\n",
        "**Learning outcomes:**\n",
        "- `Performance Comparison`: gain an understanding of the performance differences between PySpark RDD and DataFrame in various scenarios on unstructured data.\n",
        "- `Complex Task Handling`: learn how to handle complex tasks using PySpark RDD and DataFrame. The examples provided illustrate scenarios involving multiple transformations and actions, helping learners grasp the capabilities and efficiency of each approach in different use cases on unstructured data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VABP7EUIzlHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dbcd232-7926-4526-d918-a02b09a7ecbf"
      },
      "source": [
        "!pip3 install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=db29841065057eaaa810ebf7b65dbeefe71972c2c6f2951bf3b7cc7162702e77\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data in RDD and DF"
      ],
      "metadata": {
        "id": "BwAQCdWGDlLT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9eKqp92DDy_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "5da6a5be-d263-4d23-d5c6-5d5a7c0a7ca5"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark using conf\n",
        "config = SparkConf()\n",
        "config.set(\"spark.app.name\", \"RDDExample\")\n",
        "config.set(\"spark.master\", \"local[*]\")\n",
        "sc = SparkContext(config = config)\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DFExample\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6ccbe4fd19a6>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.app.name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RDDExample\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.master\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Create a SparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: SparkContext.__init__() got an unexpected keyword argument 'config'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LdYlgOOSm-on",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7443c7c-6af0-467f-e3c5-542ea72902bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Data in RDD\n"
      ],
      "metadata": {
        "id": "_T-EgTkzY3Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversion to DF\n"
      ],
      "metadata": {
        "id": "UecydeBNd0io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Count the total number of log entries"
      ],
      "metadata": {
        "id": "dGRRZtEIvj0U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jdw5sR0IvkGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Filter log entries by log level (e.g., INFO, DEBUG)"
      ],
      "metadata": {
        "id": "wOfqh4Kpnxtk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NENA0YS7nwb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Extract the first 10 timestamps from each log entry"
      ],
      "metadata": {
        "id": "6JR_V83fxX1W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AHJ4sFrLxYIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: Count the number of log entries for each log level"
      ],
      "metadata": {
        "id": "xtAFspsS7Sd8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FremR7wK7R5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5: Extract the commit information using regular expressions"
      ],
      "metadata": {
        "id": "0se1BLAjGLqR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "afeIm-1MGL-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 6: Calculate the average length of log entries\n"
      ],
      "metadata": {
        "id": "5DQSyV61IFaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "673EPhRJIFre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 7: Find the longest log entry"
      ],
      "metadata": {
        "id": "ongaEkaiISjf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "525iVm5CISHW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}